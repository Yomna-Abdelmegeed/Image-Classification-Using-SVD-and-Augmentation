{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7e75cd",
   "metadata": {},
   "source": [
    "# **Image Classification Using SVD**  \n",
    "### **Python Code for Image Classification Using Singular Value Decomposition and Optimization**\n",
    "\n",
    "#### Authors: * yomna abdelmegeed , nadia Ashraf , monica maged , yara ahmed , hagar atef , menna alla ahmed*\n",
    "\n",
    "---\n",
    "\n",
    "### Overview  \n",
    "This repository contains Python code accompanying our paper:  \n",
    "[**Image Classification Using Singular Value Decomposition and Optimization**](https://arxiv.org/pdf/2412.07288).\n",
    "\n",
    "This code demonstrates the implementation of our proposed method for image classification using singular value decomposition (SVD) and optimized categorical representative images.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc53fc",
   "metadata": {},
   "source": [
    "## Outline of the Code:\n",
    "\n",
    "\n",
    "1.   Import packages & connect to drive\n",
    "2.   Image pre-processing\n",
    "3.   Split data into training and testing\n",
    "4.   Compute templates with training set\n",
    "5.   Classification probability distribution and errors with different ranks and norms, testing against optimally weighted template\n",
    "6.   Norm evaluation at a best rank\n",
    "7.   Original test images with predicted labels for Fro norm rank 10\n",
    "8.   Generate subplot showing best rank images for one persian cat and boxer dog test for each norm\n",
    "9.   Further comparison of optimally weighted template vs average template\n",
    "10.  Single image experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03ab2f",
   "metadata": {},
   "source": [
    "## 1. Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f507f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from skimage.exposure import equalize_hist\n",
    "from skimage import io, color\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rescale\n",
    "from skimage.exposure import adjust_gamma\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from numpy.linalg import svd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7405e0c3",
   "metadata": {},
   "source": [
    "## 2. Image Pre-Processing\n",
    "### Resize and Convert to Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444a82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the folders containing images\n",
    "boxer_folder = 'boxer'\n",
    "persian_cat_folder = 'persian_cat'\n",
    "\n",
    "#image_size = (256, 256)  # Resize all images to this size\n",
    "image_size = (64, 64) \n",
    "\n",
    "# Function to load and preprocess images from a folder\n",
    "def load_images(folder_path, image_size):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.jpg', '.JPG', '.jpeg')):\n",
    "            img = io.imread(os.path.join(folder_path, filename))\n",
    "\n",
    "            # Ensure the image is RGB (3 channels) by removing an alpha channel if it exists\n",
    "            if img.shape[-1] == 4:\n",
    "                img = img[..., :3]  # Keep only the first 3 channels (RGB)\n",
    "\n",
    "            img_gray = color.rgb2gray(img)  # Convert to grayscale\n",
    "            img_resized = resize(img_gray, image_size)  # Resize\n",
    "            images.append(img_resized)  # Store resized image\n",
    "    return np.array(images)  # converts the list of images to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1473a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment images with rotations/flips and zooms (*4)\n",
    "def augment_images(images):\n",
    "    augmented = []\n",
    "    for img in images:\n",
    "        augmented.append(img)\n",
    "        augmented.append(rotate(img, angle=10))\n",
    "        augmented.append(np.fliplr(img))\n",
    "        #  zoom by 30%\n",
    "        zoomed = rescale(img, 1.3, mode='reflect', anti_aliasing=True)\n",
    "        # crop zoomed image\n",
    "        center = zoomed.shape[0] // 2\n",
    "        cropped = zoomed[center - 32:center + 32, center - 32:center + 32]\n",
    "        augmented.append(cropped)\n",
    "    return np.array(augmented)\n",
    "    \n",
    "\n",
    "# Load images\n",
    "boxer_images = load_images(boxer_folder, image_size)\n",
    "persian_cat_images = load_images(persian_cat_folder, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522af9a0",
   "metadata": {},
   "source": [
    "## 3. Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e0f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split completed:\n",
      "Persian Cat Training: (476, 64, 64)\n",
      "Persian Cat Testing: (51, 64, 64)\n",
      "Boxer Training: (476, 64, 64)\n",
      "Boxer Testing: (51, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "all_images = np.concatenate([persian_cat_images, boxer_images])\n",
    "labels = np.concatenate([np.zeros(len(persian_cat_images)), np.ones(len(boxer_images))])\n",
    "\n",
    "# Stratified split (preserve class distribution)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=123)\n",
    "train_idx, test_idx = next(sss.split(all_images, labels))\n",
    "\n",
    "# Split data\n",
    "persian_cat_train = all_images[train_idx][labels[train_idx] == 0]\n",
    "boxer_train = all_images[train_idx][labels[train_idx] == 1]\n",
    "persian_cat_test = all_images[test_idx][labels[test_idx] == 0]\n",
    "boxer_test = all_images[test_idx][labels[test_idx] == 1]\n",
    "\n",
    "# --- Added: Apply augmentation to training data only ---\n",
    "boxer_train = augment_images(boxer_train)\n",
    "persian_cat_train = augment_images(persian_cat_train)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# --- NEW: Shuffle the training data ---\n",
    "np.random.shuffle(boxer_train)\n",
    "np.random.shuffle(persian_cat_train)\n",
    "\n",
    "print(\"Training and testing split completed:\")\n",
    "print(f\"Persian Cat Training: {persian_cat_train.shape}\")  # Will show 4x original size\n",
    "print(f\"Persian Cat Testing: {persian_cat_test.shape}\")\n",
    "print(f\"Boxer Training: {boxer_train.shape}\")              # Will show 4x original size\n",
    "print(f\"Boxer Testing: {boxer_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4c38b",
   "metadata": {},
   "source": [
    "## 4. Compute templates with training set\n",
    "### Average template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9c96b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average template for each category\n",
    "def compute_template(images):\n",
    "    # Average the images to create a representative template\n",
    "    avg_image = np.mean(images, axis=0)\n",
    "    return avg_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abfc18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average template for each category\n",
    "boxer_a_template = compute_template(boxer_train)\n",
    "persian_cat_a_template = compute_template(persian_cat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9404525",
   "metadata": {},
   "source": [
    "### Optimized template using SLSQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26bbcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weighted template using optimization\n",
    "def compute_weighted_template(images, class_name=\"\"):\n",
    "    # Flatten images to vectors\n",
    "    flattened_images = images.reshape(images.shape[0], -1)\n",
    "    N = flattened_images.shape[0]\n",
    "\n",
    "    # Objective function to minimize reconstruction error for all images\n",
    "    def objective(weights):\n",
    "        weights = np.array(weights)\n",
    "        weighted_avg = np.dot(weights, flattened_images)\n",
    "        total_error = np.sum([\n",
    "            np.linalg.norm(flattened_images[i] - weighted_avg)**2 for i in range(N)\n",
    "        ])\n",
    "        return total_error\n",
    "\n",
    "    # Constraints: weights must sum to 1, and each weight >= 0\n",
    "    constraints = [\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},  # Sum of weights = 1\n",
    "        {'type': 'ineq', 'fun': lambda w: w}  # Each weight >= 0\n",
    "    ]\n",
    "    initial_weights = np.ones(N) / N  # Uniform initialization\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    result = minimize(objective, initial_weights, constraints=constraints)\n",
    "    optimized_weights = result.x\n",
    "\n",
    "    # Compute the weighted template\n",
    "    weighted_template = np.dot(optimized_weights, flattened_images).reshape(image_size)\n",
    "    print(f\"\\nCompare the Weighted and Average Template for {class_name}\")\n",
    "    print(\"Sum of difference of weights: \")\n",
    "    print(np.sum(optimized_weights - initial_weights))\n",
    "\n",
    "    return weighted_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8e898d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compare the Weighted and Average Template for Boxer Dog\n",
      "Sum of difference of weights: \n",
      "-4.85722573273506e-17\n",
      "\n",
      "Compare the Weighted and Average Template for Persian Cat\n",
      "Sum of difference of weights: \n",
      "-5.074066167232161e-17\n"
     ]
    }
   ],
   "source": [
    "# Compute the weighted average template for each category\n",
    "boxer_weighted_template = compute_weighted_template(boxer_train, \"Boxer Dog\")\n",
    "persian_cat_weighted_template = compute_weighted_template(persian_cat_train, \"Persian Cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874310a6",
   "metadata": {},
   "source": [
    "##### The above analysis shows the average and optimally weighted template are almost the same and thus we will arbitrarily continue the rest of the analysis with the optimally weighted template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559f169",
   "metadata": {},
   "source": [
    "## 5. Classification Probability Distribution and Errors with Different Ranks and Norms, Testing Against Optimally Weighted Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a1283",
   "metadata": {},
   "source": [
    "### Function to compute low rank approximations of the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9c4e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_k_approx(rank, test_images):\n",
    "   new_test_images = np.zeros(test_images.shape)\n",
    "\n",
    "\n",
    "   for i in range(test_images.shape[0]):\n",
    "     test_img = test_images[i]\n",
    "     # Compute low-rank approximation of the test image\n",
    "     U, S, VT = svd(test_img, full_matrices=False)\n",
    "     U_k = U[:, :rank]\n",
    "     S_k = np.diag(S[:rank])\n",
    "     VT_k = VT[:rank, :]\n",
    "     test_img_approx = np.dot(U_k, np.dot(S_k, VT_k))\n",
    "\n",
    "\n",
    "     new_test_images[i] = test_img_approx\n",
    "\n",
    "   return new_test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69266c77",
   "metadata": {},
   "source": [
    "### Function to store the classification margin and errors when comparing the categorical template to the low rank approximation of the test set, iterating on rank of the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bddcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rank_pred(test1, test2, temp1, temp2, rank, order = np.inf):\n",
    "  # Initialize matrices with zeros\n",
    "  E11 = np.zeros((test1.shape[0], rank))\n",
    "  E12 = np.zeros((test1.shape[0], rank))\n",
    "  E21 = np.zeros((test2.shape[0], rank))\n",
    "  E22 = np.zeros((test2.shape[0], rank))\n",
    "\n",
    "  pred1 = np.zeros((test1.shape[0], rank))\n",
    "  pred2 = np.zeros((test2.shape[0], rank))\n",
    "\n",
    "  for r in range(rank):\n",
    "    test1_k = test_k_approx(r+1, test1)\n",
    "    test2_k = test_k_approx(r+1, test2)\n",
    "    for i in range(test1_k.shape[0]):\n",
    "      E11[i,r] = np.linalg.norm(test1_k[i] - temp1, ord = order)\n",
    "      E12[i,r] = np.linalg.norm(test1_k[i] - temp2, ord = order)\n",
    "      # Predict category based on smallest error\n",
    "      if E11[i,r] < E12[i,r]:\n",
    "        pred1[i,r] = 1\n",
    "      else:\n",
    "        pred1[i,r] = 0\n",
    "    for i in range(test2_k.shape[0]):\n",
    "      E21[i,r] = np.linalg.norm(test2_k[i] - temp1, ord = order)\n",
    "      E22[i,r] = np.linalg.norm(test2_k[i] - temp2, ord = order)\n",
    "      # Predict category based on smallest error\n",
    "      if E22[i,r] < E21[i,r]:\n",
    "        pred2[i,r] = 1\n",
    "      else:\n",
    "        pred2[i,r] = 0\n",
    "\n",
    "    # Compute average along the columns, across the rows of images in each set\n",
    "    E11_mean = np.mean(E11, axis = 0) # shape should be 1 x rank\n",
    "    E12_mean = np.mean(E12, axis = 0)\n",
    "    E21_mean = np.mean(E21, axis = 0)\n",
    "    E22_mean = np.mean(E22, axis = 0)\n",
    "\n",
    "    assert E11_mean.shape == (rank,), f\"E11_mean shape expected to be {rank}, but got {E11_mean.shape}\"\n",
    "\n",
    "    assert E12_mean.shape == (rank,), f\"E12_mean shape expected to be {rank}, but got {E12_mean.shape}\"\n",
    "\n",
    "    assert E21_mean.shape == (rank,), f\"E21_mean shape expected to be {rank}, but got {E21_mean.shape}\"\n",
    "\n",
    "    assert E22_mean.shape == (rank,), f\"E22_mean shape expected to be {rank}, but got {E22_mean.shape}\"\n",
    "\n",
    "  return E11_mean, E12_mean, E21_mean, E22_mean, pred1, pred2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d94e13",
   "metadata": {},
   "source": [
    "### Function to plot the prediction probability vs rank for a given norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e006752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_probability(pred1, pred2, title):\n",
    "    \"\"\"Plots prediction probability vs rank.\n",
    "\n",
    "    Args:\n",
    "        pred1: A NumPy array (n_images, n_ranks) of predictions for the first class.\n",
    "        pred2: A NumPy array (n_images, n_ranks) of predictions for the second class.\n",
    "    \"\"\"\n",
    "\n",
    "    n_ranks = pred1.shape[1]\n",
    "    ranks = np.arange(1, n_ranks+1) # 1 to len(n_ranks)\n",
    "\n",
    "    prob1 = np.sum(pred1, axis=0) / pred1.shape[0] # sums across the columns\n",
    "    prob2 = np.sum(pred2, axis=0) / pred2.shape[0]\n",
    "    avgprob = (prob1 + prob2) / 2 # 1 by k rank array\n",
    "\n",
    "    # Identify maximum average point\n",
    "    max_value = np.max(avgprob)\n",
    "    max_index = np.argmax(avgprob) + 1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ranks, prob1, label='Boxer Dog', marker='o', color=\"green\")\n",
    "    plt.plot(ranks, prob2, label='Persian Cat', marker='o', color=\"orange\")\n",
    "    plt.plot(ranks, avgprob, label='Average', marker='x', color=\"black\")\n",
    "\n",
    "    # Plot maximum average point\n",
    "    plt.plot(max_index, max_value, 'ro', label=f'Maximum ({max_index}, {max_value:.2f})')\n",
    "\n",
    "    plt.xlabel(\"Rank\", fontsize=20)\n",
    "    plt.ylabel(\"Prediction Probability\", fontsize=20)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xticks(fontsize=17)\n",
    "    plt.yticks(fontsize=17)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aebff7",
   "metadata": {},
   "source": [
    "### Function to plot the error vs rank for a given norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af6a9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errorvsrank(E11_w_mean, E12_w_mean, E21_w_mean, E22_w_mean, ord):\n",
    "  n_ranks = len(E11_w_mean)\n",
    "  ranks = range(1, n_ranks+1) # 1 to len(n_ranks)\n",
    "\n",
    "  plt.figure(figsize=(6, 4))\n",
    "  plt.plot(ranks, E11_w_mean, label='Error with Boxer Dog Template', color = 'green')\n",
    "  plt.plot(ranks, E12_w_mean, label='Error with Persian Cat Template', color = 'lime')\n",
    "  plt.xlabel(\"Rank\", fontsize=15)\n",
    "  plt.ylabel(\"Error\", fontsize=15)\n",
    "  plt.title(f\"Boxer Dog Test Images vs Rank for Norm: {ord}\", fontsize=16)\n",
    "  plt.xticks(fontsize=13)\n",
    "  plt.yticks(fontsize=13)\n",
    "  plt.grid(True)\n",
    "  plt.legend(fontsize=13)\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure(figsize=(6, 4))\n",
    "  plt.plot(ranks, E21_w_mean, label='Error with Boxer Dog Template', color = 'orangered')\n",
    "  plt.plot(ranks, E22_w_mean, label='Error with Persian Cat Template', color = 'orange')\n",
    "  plt.xlabel(\"Rank\", fontsize=15)\n",
    "  plt.ylabel(\"Error\", fontsize=15)\n",
    "  plt.title(f\"Persian Cat Test Images vs Rank for Norm: {ord}\", fontsize=16)\n",
    "  plt.xticks(fontsize=13)\n",
    "  plt.yticks(fontsize=13)\n",
    "  plt.grid(True)\n",
    "  plt.legend(fontsize=13)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224e6de",
   "metadata": {},
   "source": [
    "### Function to generate test and pred for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53db69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_pred(pred1, pred2):\n",
    "    # Initialize y_test arrays\n",
    "    y_test1 = np.ones_like(pred1)  # Actual is Class 1\n",
    "    y_test2 = np.full_like(pred2, 2)  # Actual is Class 2\n",
    "\n",
    "    # Generate pred_one and pred_two\n",
    "    pred_one = np.where(pred1 == 1, 1, 2)  # Class 1 if pred1 == 1, else Class 2\n",
    "    pred_two = np.where(pred2 == 1, 2, 1)  # Class 2 if pred2 == 1, else Class 1\n",
    "\n",
    "    # Combine predictions and ground truth\n",
    "    pred = np.concatenate((pred_one, pred_two))\n",
    "    y_test = np.concatenate((y_test1, y_test2))\n",
    "\n",
    "    return pred, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95749558",
   "metadata": {},
   "source": [
    "### Function to generate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45366757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(y_test, pred, fixed_rank, ord):\n",
    "    # Generate and plot confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, pred)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar=False,\n",
    "        xticklabels=[\"Boxer Dog\", \"Persian Cat\"],\n",
    "        yticklabels=[\"Boxer Dog\", \"Persian Cat\"],\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"black\",\n",
    "        annot_kws={\"fontsize\": 13, \"weight\": \"bold\"}\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Labels\", fontsize=14)\n",
    "    plt.ylabel(\"True Labels\", fontsize=14)\n",
    "    plt.title(f\"Confusion Matrix for fixed rank = {fixed_rank}, and norm = {ord}\", fontsize=16)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a548b",
   "metadata": {},
   "source": [
    "### Iterate on the norms and produce results from functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce811e6",
   "metadata": {},
   "source": [
    "#### Analysis with training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recalls dictionary with accuracy\n",
    "metrics = {'1': {'TP Recall': [], 'FP Recall': [], 'Accuracy': []},\n",
    "           '2': {'TP Recall': [], 'FP Recall': [], 'Accuracy': []},\n",
    "           'inf': {'TP Recall': [], 'FP Recall': [], 'Accuracy': []},\n",
    "           'fro': {'TP Recall': [], 'FP Recall': [], 'Accuracy': []}}\n",
    "\n",
    "for ord in [1,2,np.inf,'fro']:\n",
    "  E11_w_mean, E12_w_mean, E21_w_mean, E22_w_mean, pred1_w, pred2_w = generate_rank_pred(test1=boxer_train, test2=persian_cat_train, temp1=boxer_weighted_template, temp2=persian_cat_weighted_template, rank = 60, order = ord)\n",
    "  fixed_rank = plot_prediction_probability(pred1_w, pred2_w, title = f\"Prediction Probability vs. Rank using Weighted Template for Norm: {ord}\") # this is where we specify that class 1 is boxer, class 2 is persian cat\n",
    "\n",
    "  # Generate pred and y_test arrays\n",
    "  pred1 = pred1_w[:,fixed_rank-1]\n",
    "  pred2 = pred2_w[:,fixed_rank-1]\n",
    "  pred, y_test = generate_test_pred(pred1, pred2)\n",
    "\n",
    "  # Generate and plot confusion matrix\n",
    "  generate_confusion_matrix(y_test, pred, fixed_rank, ord)\n",
    "\n",
    "  # Generate classification report\n",
    "  class_report = classification_report(y_test, pred, target_names=['Boxer Dog', 'Persian Cat'], output_dict=True)\n",
    "\n",
    "  # Map np.inf to 'inf' for dictionary key compatibility\n",
    "  ord_key = 'inf' if ord == np.inf else str(ord)\n",
    "\n",
    "  # Store recalls and accuracy\n",
    "  metrics[ord_key]['TP Recall'].append(class_report['Boxer Dog']['recall'])\n",
    "  metrics[ord_key]['FP Recall'].append(1 - class_report['Boxer Dog']['recall'])  # False positive recall is 1 - TP Recall\n",
    "  metrics[ord_key]['Accuracy'].append(class_report['accuracy'])  # Store accuracy\n",
    "\n",
    "  # Display results\n",
    "  plot_errorvsrank(E11_w_mean, E12_w_mean, E21_w_mean, E22_w_mean, ord)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
